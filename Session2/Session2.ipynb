{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Time series analysis\n",
    "\n",
    "This material was prepared for the Australian Water School by Chris Turnadge (chris.turnadge@csiro.au) and was presented on 10 June 2021.\n",
    "\n",
    "The second session of the Python course is focused on the analysis of hydrological and geoscientific time series data. Topics covered include data wrangling tools specific to time series analysis, convolution and deconvolution, and harmonic analysis. We'll illustrate this with hydrological and geoscientific datasets. The third session of the course next week will look in greater detail into the analysis and interpolation of spatial datasets, including combining Python with QGIS.\n",
    "\n",
    "# 0.1. Import packages\n",
    "\n",
    "First, we need to import all of the Python packages that we'll need for this session.\n",
    "\n",
    "These are:\n",
    "* <b>numpy</b> for numerical computation - including linear algebra operations, and a Fourier transform routine.\n",
    "* <b>scipy</b> for scientific computation - including a host of optimisation algorithms, and a host of signal processing routines.\n",
    "* <b>pandas</b> for importing and processing time series data.\n",
    "* <b>matplotlib</b> for creating output plots.\n",
    "* <b>pyplot</b>, because it is often handy to call pyplot directly, rather than via matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2. Create default plotting function for time domain plots\n",
    "\n",
    "Next, we'll create a customised plotting function for the plotting of time series data in the time (as opposed to frequency) domain. This is partly to change some of the plotting defaults to make plots that are more appropriate to time series data; e.g. increasing the widths of plots. It also means we can have a consistent look when creating many plots. The first code block changes some of the default settings for matplotlib. The second code block defines a function that we can call each time we create a new figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams[ 'font.sans-serif'  ] = 'Calibri'\n",
    "mpl.rcParams[ 'font.size'        ] = 9\n",
    "mpl.rcParams[ 'mathtext.default' ] = 'regular'          \n",
    "mpl.rcParams[ 'xtick.direction'  ] = 'out'\n",
    "mpl.rcParams[ 'ytick.direction'  ] = 'out'       \n",
    "mpl.rcParams[ 'lines.linewidth'  ] = 1.0     \n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "def create_formatted_plot(xlabel, ylabel, title=''):\n",
    "    f,s = plt.subplots(figsize=[16.00/2.54, 6.00/2.54])\n",
    "    s.set_title(title, loc='left', fontsize=9)\n",
    "    s.set_xlabel(xlabel)\n",
    "    s.set_ylabel(ylabel)\n",
    "    s.spines['top'].set_visible(False)\n",
    "    s.spines['right'].set_visible(False)\n",
    "    s.grid(which='major', axis='both', c='silver', ls='-', lw=0.5)                            \n",
    "    s.grid(which='minor', axis='both', c='silver', ls='-', lw=0.5)                            \n",
    "    s.ticklabel_format(useOffset=False)\n",
    "    plt.tight_layout()\n",
    "    return f,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. Data wrangling for time series data\n",
    "\n",
    "We'll kick things off with a bit of data wrangling specific to time series data. We'll read a CSV dataset into a dataframe object named \"df\". Pandas allows data to be indexed using \"datetimes\", instead of the usual sequence of integers (i.e. 0, 1, 2, etc.) The <i>\"parse_dates\"</i> keyword tells pandas that we want to use datetimes as indices, and the <i>\"index_col\"</i> keyword identifies the column in the CSV file that contains the datetime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pahute = pd.read_csv('Pahute_Mesa_example_dataset_v01_260221_edited.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the raw data: (which are also located in the \"Session2\" folder on the GitHub repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pahute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the data for visual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='data')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the dataset for NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pahute[pahute.GW.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these in red, to identify where the gaps occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='data')\n",
    "for dt in pahute.index[pahute.GW.isna()]:\n",
    "    s.plot(dt, pahute.GW.mean(), 'o', c='C1')\n",
    "s.plot(dt, pahute.GW.mean(), 'o', c='C1', label='gaps')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can zoom into sections of the time series that contains gaps. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(a)')\n",
    "s.plot(pahute.GW['2011-05-22':'2011-05-23'], label='data')\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(b)')\n",
    "s.plot(pahute.GW['2011-07-13':'2011-07-14'], label='data')\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(c)')\n",
    "s.plot(pahute.GW['2011-09-03':'2011-09-04'], label='data')\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(d)')\n",
    "s.plot(pahute.GW['2011-10-25':'2011-10-26'], label='data')\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove these using the <i>dropna</i> command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute.dropna(inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check for the presence of duplicated datetime values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2[pahute2.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove these using the <i>duplicated</i> command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute2[~pahute2.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data appear to be sampled every five minutes, but because we removed all of the NaN values, the data are no longer  sampled uniformly in time. To do this, we create a new sequence of datetimes at a five minute frequency. Then we extract all data from our dataframe at these new datetimes by using the <i>\"reindex\"</i> command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pahute3 = pahute2.reindex(pd.date_range(start=pahute.index[0], end=pahute.index[-1], freq='600S'))\n",
    "pahute3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check for the presence of NaN values in this new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pahute3[pahute3.GW.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Gap filling\n",
    "\n",
    "Now that we have identified that we do have gaps in our data, and identified at which datetimes these gaps occurred, we can fill these gaps. Gap filling can be a very tedious, repetitive and time consuming task when performed using software like Microsoft Excel. Instead, gap filling can be performed in a single line of Python code. A scripted approach to gap filling can also help workflows to be run from start to finish, which makes them repeatable. This is often needed, since our work typically involves multiple iterations.\n",
    "\n",
    "The simplest method of interpolation, and which I most commonly use, is linear interpolation. This is suitable for relatively short data gaps. Note that this method assumes that data are uniformly spaced in time. If this is nnot the case, the keyword <i>method=\"time\"</i> can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute = pd.read_csv('Pahute_Mesa_example_dataset_v01_260221_edited.csv', index_col=0, parse_dates=True)\n",
    "pahute = pahute[~pahute.index.duplicated(keep='first')]\n",
    "\n",
    "pahute2 = pahute.interpolate(method='linear', inplace=False) # 'time', 'polynomial', 'spline'\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(a)')\n",
    "s.plot(pahute.GW ['2011-05-22':'2011-05-23'], label='data', zorder=2)\n",
    "s.plot(pahute2.GW['2011-05-22':'2011-05-23'], label='linear', zorder=1)\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(b)')\n",
    "s.plot(pahute.GW ['2011-07-13':'2011-07-15'], label='data', zorder=2)\n",
    "s.plot(pahute2.GW['2011-07-13':'2011-07-15'], label='linear', zorder=1)\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(c)')\n",
    "s.plot(pahute.GW ['2011-09-03':'2011-09-05'], label='data', zorder=2)\n",
    "s.plot(pahute2.GW['2011-09-03':'2011-09-05'], label='linear', zorder=1)\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(d)')\n",
    "s.plot(pahute.GW ['2011-10-25':'2011-10-27'], label='data', zorder=2)\n",
    "s.plot(pahute2.GW['2011-10-25':'2011-10-27'], label='linear', zorder=1)\n",
    "plt.legend(loc=0, ncol=2)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often data are sampled at a resolution that is unsuitable for analysis: \n",
    "* If data are sampled too frequently, downsampling can be used to reduce the resolution, and the number of samples.\n",
    "\n",
    "\n",
    "* Alternatively, if data are sampled at a low resolution, upsampling (with interpolation) can be used to increase the resolution, and the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Downsampling\n",
    "\n",
    "For example, suppose we wish to downsample our data, which were sampled at 15 minute frequency, to a (lower) hourly frequency. We can achieve this using the <i>\"resample\"</i> command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute.resample('1H').first() # or mean()\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.GW ['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=6, linewidth=2, label='original')\n",
    "s.plot(pahute2.GW['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=3, label='downsampled')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "s.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Upsampling\n",
    "\n",
    "Alternatively, suppose we wish to upsample our data to a (higher) five minute frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute.resample('5T').ffill() # or bfill\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.GW ['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=6, linewidth=2, label='original')\n",
    "s.plot(pahute2.GW['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=3, label='downsampled')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is problematic, since the forward and backward filling cannot interpolate between measured points. Instead, we can use a combination of reindexing and interpolation. Reindexing in this way can also be useful for correcting logger drift; i.e. when internal clocks drift off of a set sampling frequency. Reindexing can be used to interpolate values at intended sampling times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute.reindex(pd.date_range(start=pahute.index[0], end=pahute.index[-1], freq='5T'))\n",
    "pahute2 = pahute2.interpolate(method='linear', inplace=False)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.GW ['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=6, linewidth=2, label='original')\n",
    "s.plot(pahute2.GW['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=3, label='downsampled')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Detrending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often detrending of measured data is required before further analysis. Detrending can serve to remove low frequency, large period trends, such as seasonal effects. Or it can be used to remove high frequency, small period trends, such as random noise. Detrending can be performed in the time or frequency domain, and we will examine both types of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Time domain methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.1 Moving average filtering\n",
    "\n",
    "My use of detrending in the time domain has been limited to moving average filtering. While this can be applied in Excel (for example), it is really easy to test the effects of varying window sizes when implemented using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute2 = pahute.rolling(window=2, win_type='boxcar', center=True).mean().dropna()\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(a)')\n",
    "s.plot(pahute.GW ['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=6, linewidth=2, label='original')\n",
    "s.plot(pahute2.GW['2011-05-22 00:00:00':'2011-05-22 6:00:00'], 'o-', markersize=3, label='downsampled')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m\\n%H:%M'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure', title='(b)')\n",
    "s.plot(pahute.GW-pahute2.GW, label='residual')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Frequency domain methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency domain methods provide opportunities to filter data in a variety of ways:\n",
    "\n",
    "* <b>High-pass</b> filtering can be used to exclude signals with relatively <b>low frequencies</b> / small periods --  for example, seasonal trends. \n",
    "\n",
    "\n",
    "* <b>Low-pass</b> filtering can be used to exclude signals with relatively <b>high frequencies</b> / large periods -- for example, random noise.\n",
    "\n",
    "\n",
    "* <b>Band-pass</b> filtering can be used to exclude signals that occur <b>outside</b> a given frequency range.\n",
    "\n",
    "\n",
    "* <b>Band-stop</b> filtering can be used to exclude signals that occur <b>within</b> a given frequency range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can look at the effects of these filters, we'll need to define a routine to calculate amplitude spectra. We will calculate these using the Fourier transform. This method assumes that a given time series can be decomposed into a finite number of sinusoidal signals which, when summed, result in an approximation of the time series of interest. The number of frequencies that can be estimated from a given time series is a function of its length. The maximum frequency is a function of the sampling frequency used when measuring a time series. The Fourier transform is used to calculate the amplitude and phase parameters of each frequency that is present.\n",
    "\n",
    "We'll define our Fourier transform routine as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(data, Fs):\n",
    "    n = len(data)\n",
    "    T = n/Fs\n",
    "    k = np.arange(n)\n",
    "    F = k/T    \n",
    "    F = F[:n/2]\n",
    "    A = 2.*(2./n)*np.abs(np.fft.fft(data*np.hanning(n)))[:n/2]\n",
    "    return F,A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to define a new type of plot, which we will use when we plot the amplitudes of each frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_domain_plot(f, s, X, Y, c='C0', title=''):\n",
    "    s.set_title(title)\n",
    "    s.plot(X, Y, 'o', c=c, markersize=3, markerfacecolor='none')\n",
    "    for x,y in zip(X,Y):\n",
    "        s.plot([x,x], [0.,y], '-', c=c)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll reimport the Pahute Mesa groundwater pressure data, before (1) removing duplicates, (2) removing NaN values, (3) reindexing the dataset, and (4) using a subset of the data to improve the clarity of the following figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute = pd.read_csv('Pahute_Mesa_example_dataset_v01_260221_edited.csv', index_col=0, parse_dates=True)\n",
    "pahute = pahute[~pahute.index.duplicated(keep='first')]\n",
    "pahute = pahute.reindex(pd.date_range(start=pahute.index[0], end=pahute.index[-1], freq='15T'))\n",
    "pahute.interpolate(method='linear', inplace=True)\n",
    "pahute = pahute[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For frequency domain analyses, we need a dataset that has a zero mean value. One way of achieving this is to subtract the mean value from the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pahute.GW = pahute.GW-pahute.GW.mean()\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='original')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d-%b'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we'll consider Earth tide influences on the Pahute Mesa groundwater pressure time series. We'll focus on frequencies of up to two cycles per day, as this range includes the 5-12 dominant frequencies. We can define this range as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.1. Low-pass filtering\n",
    "\n",
    "Here we exclude <b>high</b> frequencies and retain <b>low</b> frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sos = sc.signal.butter(N=7, Wn=1.0, btype='lowpass', fs=24., output='sos')\n",
    "pahute2 = sc.signal.sosfiltfilt(sos, np.array(pahute.GW))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude')\n",
    "F,A = calculate_fft(pahute.GW, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C0')\n",
    "F,A = calculate_fft(pahute2, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C1')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='original')\n",
    "s.plot(pahute.index, pahute2,   label='low-pass filtered')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d-%b'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2. High-pass filtering\n",
    "\n",
    "Here we exclude <b>low</b> frequencies and retain <b>high</b> frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = sc.signal.butter(N=7, Wn=1.0, btype='highpass', fs=24., output='sos')\n",
    "pahute2 = sc.signal.sosfiltfilt(sos, np.array(pahute.GW))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude')\n",
    "F,A = calculate_fft(pahute.GW, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C0')\n",
    "F,A = calculate_fft(pahute2, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C1')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='original')\n",
    "s.plot(pahute.index, pahute2, label='high-pass filtered')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d-%b'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.3. Band-pass filtering\n",
    "\n",
    "Here we <b>exclude</b> all frequencies <b>except</b> within a defined frequency band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = sc.signal.butter(N=7, Wn=[1.0, 1.5], btype='bandpass', fs=24., output='sos')\n",
    "pahute2 = sc.signal.sosfiltfilt(sos, np.array(pahute.GW))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude')\n",
    "F,A = calculate_fft(pahute.GW, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C0')\n",
    "F,A = calculate_fft(pahute2, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C1')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='original')\n",
    "s.plot(pahute.index, pahute2, label='band-pass filtered')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d-%b'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.3. Band-stop filtering\n",
    "\n",
    "Here we <b>retain</b> all frequencies <b>except</b> within a defined frequency band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = sc.signal.butter(N=7, Wn=[1.0, 1.5], btype='bandstop', fs=24., output='sos')\n",
    "pahute2 = sc.signal.sosfiltfilt(sos, np.array(pahute.GW))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude')\n",
    "F,A = calculate_fft(pahute.GW, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C0')\n",
    "F,A = calculate_fft(pahute2, Fs=24.)\n",
    "frequency_domain_plot(f, s, F[(F>0.5)&(F<3.1)], A[(F>0.5)&(F<3.1)], c='C1')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Pressure')\n",
    "s.plot(pahute.index, pahute.GW, label='original')\n",
    "s.plot(pahute.index, pahute2, label='band-stop filtered')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d-%b'))\n",
    "plt.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. Interpreting responses to time-lagged signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various hydrological processes can be characterised as functions of time lag. For example, both groundwater levels and surface water flows can exhibit time-lagged responses to precipitation. In the following worked examples, we will use a precipitation dataset as the driving process: \n",
    "\n",
    "* We will see how convolution can be used to combine a known input with a known response function to result in a time-lagged and diminished signal. \n",
    "\n",
    "\n",
    "* We'll use deconvolution to inversely estimate a response function when input and output time series are both measured. \n",
    "\n",
    "We'll finish with a demonstration of the application of deconvolution to estimate a response function that characterises groundwater pressure responses to barometric pressure fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip = pd.read_csv('HM01X_Data_023885_999999999794987_edited.csv', index_col=0, parse_dates=True, \n",
    "                     dayfirst=True, header=0, names=['Datetime', 'PR'])\n",
    "precip[precip>10.] = 0.\n",
    "precip = precip[~precip.index.duplicated(keep='first')]\n",
    "precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Precipitation (mm)')\n",
    "s.plot(precip.index, precip.PR, label='precip')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Correlation in the time domain: Cross-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip2 = precip.rolling(window=100, win_type='boxcar', center=True).mean().dropna()\n",
    "pr = np.array(precip2.PR)\n",
    "c = sc.signal.correlate(pr[100:], pr[:-100], mode='same')\n",
    "f,s = create_formatted_plot(xlabel='Count', ylabel='Cross-correlation (-)')\n",
    "dt = 15./60./24.\n",
    "t  = np.arange(0., len(c)*dt, dt) \n",
    "s.plot(t, c)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Response kernel types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thes include impulse response, lagged impulse response, exponential decrease, lagged exponential response, and logarithmic increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags = 24\n",
    "dt = 0.5 # hours\n",
    "tl = np.arange(nlags)*dt\n",
    "\n",
    "gt = np.zeros(nlags)\n",
    "gt[0] = 1.\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)', \n",
    "                            title='(a) Impulse response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "gt = np.zeros(nlags)\n",
    "gt[6] = 1.\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)',\n",
    "                            title='(b) Delayed impulse response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "gt = 1.5**-tl\n",
    "gt = gt/np.sum(gt)\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)',\n",
    "                            title='(c) Exponentially decreasing response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "gt = np.concatenate([np.zeros(6), 1.8**-tl[:-6]])\n",
    "gt = gt/np.sum(gt)\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)',\n",
    "                            title='(d) Delayed exponentially decreasing response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "gt = np.log10(tl+1)*4.\n",
    "gt = gt/np.sum(gt)\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)',\n",
    "                            title='(e) Logarithmically increasing response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Convolution (in the forwards direction)\n",
    "\n",
    "To provide a worked example of a convolution model based on real data, here we convolute the precipitation time series with a known response function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = np.array(precip2.PR)\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Precipitation (mm)', \n",
    "                            title='(a) Input dataset:Time series of precipitation')\n",
    "s.plot(precip.index, precip.PR, label='precip')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "\n",
    "nlags = 24\n",
    "dt = 0.5\n",
    "tl = np.arange(nlags)*dt\n",
    "gt = np.concatenate([np.zeros(6), 1.8**-tl[:-6]])\n",
    "gt = gt/np.sum(gt)*0.5\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)',\n",
    "                            title='(b) Response kernel: delayed exponentially decreasing response')\n",
    "frequency_domain_plot(f, s, tl, gt)\n",
    "\n",
    "out = np.convolve(pr, gt[::-1], 'same')\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Precipitation (mm)', \n",
    "                            title='(c) Convolution model response (orange) to input (blue) '+\n",
    "                            'via response function (above)')\n",
    "s.plot(precip2.index, pr)\n",
    "s.yaxis.label.set_color('C0')\n",
    "s.tick_params(axis='y', colors='C0')\n",
    "s2 = s.twinx()\n",
    "s2.plot(precip2.index, out, c='C1')\n",
    "s2.set_ylabel('Response (mm)')\n",
    "s2.yaxis.label.set_color('C1')\n",
    "s2.tick_params(axis='y', colors='C1')\n",
    "s2.set_ylim(s.get_ylim())\n",
    "s2.spines['top'].set_visible(False)\n",
    "s.set_xlim('2010-10-1', '2010-12-1')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%m'))\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Deconvolution (in the backwards direction)\n",
    "\n",
    "In practice, as hydrologists we typically observe the input and output time series, and use these to characterise the system response. In a linear system, all responses are of the same magnitude, regardless of either (a) the magnitude of the input event or (b) the time between input events. The response of a linear system can be characterised using a response kernel, which can be estimated via deconvolution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(gt):\n",
    "    global pr, out\n",
    "    return np.sum((np.convolve(pr, gt[::-1], 'same')-out)**2.)\n",
    "\n",
    "gt2 = sc.optimize.least_squares(objective_function, np.zeros(nlags)).x\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)')\n",
    "frequency_domain_plot(f, s, tl, gt,  c='C0')\n",
    "frequency_domain_plot(f, s, tl, gt2, c='C1')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Example application: Deconvolution of a groundwater level time series\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_deconvolution(gw, ba):\n",
    "    dt = 1./24.\n",
    "    t  = np.arange(0., dt*len(gw)+dt, dt)\n",
    "    d2 = gw.index   \n",
    "    y  = gw.Pressure_kPa\n",
    "    x  = ba.Pressure_kPa\n",
    "    dx = np.diff(x)/dt\n",
    "    dy = np.diff(y)/dt\n",
    "    nlag = 48 \n",
    "    n = len(dx)\n",
    "    nn = range(n) \n",
    "    lags = np.arange(nlag+1)\n",
    "    nm = nlag+1\n",
    "    v = np.zeros([n, nm])\n",
    "    for i in range(nm):\n",
    "        j = lags[i]\n",
    "        k = np.arange(n-j)\n",
    "        v[j+k, i] = dx[k] \n",
    "    u1 = np.zeros([n, 10])\n",
    "    u2 = u1.copy()\n",
    "    X = np.hstack([v, u1, u2])\n",
    "    Z = np.hstack([np.ones([n,1]), v])\n",
    "    c = np.linalg.lstsq(Z, dy, rcond=None)[0]\n",
    "    brf = c[np.arange(1, nm+1)]\n",
    "    return lags, brf\n",
    "\n",
    "gw = pd.read_csv('Norfolk_GW.csv', index_col=0, parse_dates=True)\n",
    "ba = pd.read_csv('Norfolk_BA.csv', index_col=0, parse_dates=True)\n",
    "lags, brf = regression_deconvolution(gw, ba)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)')\n",
    "s.plot(lags, brf, 'o-', markersize=3, markeredgecolor='C0', markerfacecolor='none', label='BRF')\n",
    "s.legend(loc=0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric models can subsequently be fitted to response functions, in order to obtain estimates of hydraulic properties. For example, a confined aquifer response can be characterised using a exponentially decreasing kernel function. We can fit this using a least squares optimisation algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(B):\n",
    "    global lags, brf\n",
    "    return np.sum((np.exp(-B*lags)*brf[0]-brf)**2.)\n",
    "\n",
    "B = sc.optimize.least_squares(objective_function, [1.]).x\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Time lag (hours)', ylabel='Response (fraction of total)')\n",
    "s.plot(lags, brf, 'o-', markersize=3, markeredgecolor='C0', markerfacecolor='none', label='BRF')\n",
    "s.plot(lags, np.exp(-B*lags)*brf[0], '--', c='C1', label='model (B='+str('%.3f'% B[0])+')')\n",
    "s.legend(loc=0, ncol=2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Identifying responses to periodic signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various hydrological processes respond to processes that are periodic(i.e. cyclic). For example, groundwater pressures respond to periodic variations in gravity at the Earth's surface. These are driven by the combined movements of the Sun, Moon and planets, which are known as Earth tides. Similarly, both groundwater pressures and estuary stage heights respond to ocean tides.\n",
    "\n",
    "In the following worked examples, we will look at the presence of periodic signals in groundwater and barometric pressure time series. We'll look at the variety of methods available in the SciPy package for identifying dominant frequencies and to quantify their amplitudes.   \n",
    "\n",
    "We'll finish with a demonstration that combines two frequency domain methods to characterise aquifer confinement status, based on groundwater pressure responses to barometric pressure fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pahute = pd.read_csv('Pahute_Mesa_example_dataset_v01_260221_edited.csv', index_col=0, \n",
    "                     parse_dates=True).resample('1H').first()[:1000]\n",
    "pahute.GW = pahute.GW-pahute.GW.mean()\n",
    "pahute.BA = pahute.BA-pahute.BA.mean()\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Magnitude', title='(a) Groundwater')\n",
    "s.plot(pahute.GW)\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Date', ylabel='Magnitude', title='(b) Barometric')\n",
    "s.plot(pahute.BA, c='C1')\n",
    "s.xaxis.set_major_formatter(mpl.dates.DateFormatter('%b\\n%Y'))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F,A = calculate_fft(np.array(pahute.GW), Fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude', title='(a) Groundwater')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], A[(F>0.7)&(F<2.1)])\n",
    "plt.legend(loc=0)\n",
    "\n",
    "F,A = calculate_fft(np.array(pahute.BA), Fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude', title='(b) Barometric')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], A[(F>0.7)&(F<2.1)], c='C1')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Correlation in the frequency domain: Coherence\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, coh = sc.signal.coherence(np.array(pahute.GW), np.array(pahute.BA), fs=24.)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Coherence')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], 1.-coh[(F>0.7)&(F<2.1)])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Cross-spectral density\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, csd = sc.signal.csd(np.array(pahute.GW), np.array(pahute.BA), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Cross spectral density')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], csd[(F>0.7)&(F<2.1)])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Periodograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, psd = sc.signal.periodogram(np.array(pahute.GW), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Power spectral density', \n",
    "                            title='(a) Groundwater')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], psd[(F>0.7)&(F<2.1)])\n",
    "\n",
    "F, psd = sc.signal.periodogram(np.array(pahute.BA), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Power spectral density', \n",
    "                            title='(b) Barometric')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], psd[(F>0.7)&(F<2.1)], c='C1')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, psd = sc.signal.welch(np.array(pahute.GW), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Power spectral density', title='(a) Groundwater')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], psd[(F>0.7)&(F<2.1)])\n",
    "\n",
    "F, psd = sc.signal.welch(np.array(pahute.BA), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Power spectral density', \n",
    "                            title='(b) Barometric')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], psd[(F>0.7)&(F<2.1)], c='C1')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. Lomb-Scargle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [np.random.uniform(0.7, 2.1) for i in range(30)]\n",
    "psd = sc.signal.lombscargle(np.arange(len(np.array(pahute.GW))), np.array(pahute.GW), freqs=F)\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Power spectral density')\n",
    "frequency_domain_plot(f, s, F, psd)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4. Harmonic least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hals(ft, fs):\n",
    "    t = np.arange(len(ft))*fs\n",
    "    freqs = np.array([0.929536, 0.966446, 1.000000, 1.895982, 1.932274, 2.000000])\n",
    "    N = len(ft)\n",
    "    f = 2.*np.pi*freqs\n",
    "    num_freqs = len(f)\n",
    "    phi = np.empty([N, 2*num_freqs+1])\n",
    "    for j in range(num_freqs):\n",
    "        phi[:,2*j]   = np.cos(f[j]*t)\n",
    "        phi[:,2*j+1] = np.sin(f[j]*t)\n",
    "    phi[:,-1] = 1\n",
    "    theta, ssr, rank, singular = np.linalg.lstsq(phi, ft, rcond=None)    \n",
    "    Z = theta[:-1:2] + theta[1:-1:2]*1j\n",
    "    amplitudes = np.abs(Z)\n",
    "    phases = np.angle(Z)\n",
    "    return freqs, amplitudes, phases\n",
    "\n",
    "F,A,P = hals(np.array(pahute.GW), fs=24.)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Amplitude')\n",
    "frequency_domain_plot(f, s, F, A)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Example application: Estimation of aquifer specific storage from calculated amplitude values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ft = np.array(df.GW)[:1000]-df.GW[:1000].mean()\n",
    "F = np.fft.fftfreq(n=len(ft), d=1.)\n",
    "Z = np.fft.fft(ft) #np.hanning(len(ft))*ft))\n",
    "F = F[:len(Z)/2]\n",
    "Z = Z[:len(Z)/2]\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Amplitude')\n",
    "frequency_domain_plot(f, s, F[F>0.1], np.abs(Z)[F>0.1])\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Phase (radians)')\n",
    "s.plot(F, Z.imag)\n",
    "\n",
    "f,s = create_formatted_plot(xlabel='Amplitude (mm)', ylabel='Phase (radians)')\n",
    "s.plot(Z.real, Z.imag)\n",
    "'''\n",
    "\n",
    "pahute = pd.read_csv('Pahute_Mesa_example_dataset_v01_260221_edited.csv', index_col=0, \n",
    "                     parse_dates=True).resample('1H').first()[:1000]\n",
    "pahute.GW = pahute.GW-pahute.GW.mean()\n",
    "pahute.BA = pahute.BA-pahute.BA.mean()\n",
    "\n",
    "F, csd = sc.signal.csd(np.array(pahute.GW), np.array(pahute.BA), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency (cycles per day)', ylabel='Cross spectral density',\n",
    "                            title='(a) Cross spectral density of groundwater and barometric pressure')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], csd[(F>0.7)&(F<2.1)], c='C0')\n",
    "ylim = s.get_ylim()\n",
    "\n",
    "F, psd = sc.signal.welch(np.array(pahute.GW), fs=24.)\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Power spectral density',\n",
    "                            title='(b) Power spectral density of groundwater pressure')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], psd[(F>0.7)&(F<2.1)], c='C1')\n",
    "s.set_ylim(ylim)\n",
    "\n",
    "BE = csd/psd\n",
    "f,s = create_formatted_plot(xlabel='Frequency', ylabel='Barometric efficiency',\n",
    "                            title='(c) Frequency-dependent groundwater barometric efficiency')\n",
    "frequency_domain_plot(f, s, F[(F>0.7)&(F<2.1)], BE[(F>0.7)&(F<2.1)], c='C2')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. HydroGeoSines\n",
    "\n",
    "To conclude, I'd just like to talk briefly about HydroGeoSines.\n",
    "\n",
    "Many of the methods demonstrated today have been incorporated into the Python package \"HydroGeoSines\", which is available from GitHub here:\n",
    "\n",
    "https://github.com/HydroGeoSines/HydroGeoSines\n",
    "\n",
    "Currently, two example notebooks are available, which demonstrate the capabilities of the package:\n",
    "\n",
    "https://github.com/HydroGeoSines/HydroGeoSines/blob/master/examples/Notebooks/Groundwater_head_correction.ipynb\n",
    "\n",
    "https://github.com/HydroGeoSines/HydroGeoSines/blob/master/examples/Notebooks/Estimation_of_K_Ss_BE.ipynb\n",
    "\n",
    "This package is intended to make these methods of interpretation more accessible, as well as providing data wrangling tools to simplify the cleaning of time series data prior to analysis. I am one of three researchers who are actively involved in developing HydroGeoSines, which is still in its early stages. Please get in touch if you would like any further information, especially if you believe the package would be of benefit to your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
